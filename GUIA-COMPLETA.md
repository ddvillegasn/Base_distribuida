# ğŸ—„ï¸ GuÃ­a Completa - Base de Datos Distribuida con Sharding

## ğŸ“‹ Â¿QuÃ© es este proyecto?

Es una **base de datos clave-valor distribuida** que reparte los datos entre mÃºltiples computadoras (nodos). Cada nodo almacena solo una parte de los datos, no todo.

---

## ğŸ—ï¸ Arquitectura y Conceptos

### 1ï¸âƒ£ **Sharding (Particionamiento)**
Los datos se dividen entre 4 nodos usando un **hash de la clave**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cliente hace: SET nombre=Juan      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Hash("nombre") % 4  â”‚  â†’ Resultado: 2
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ PC 1    â”‚ PC 2   â”‚ PC 3   â”‚ PC 4â”‚
    â”‚ Mumbai  â”‚ Delhi  â”‚Chennai â”‚Banglâ”‚
    â”‚ Shard 0 â”‚Shard 1 â”‚Shard 2 â”‚Shardâ”‚
    â”‚         â”‚        â”‚  âœ“     â”‚  3  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           "nombre=Juan" se guarda en Chennai (Shard 2)
```

### 2ï¸âƒ£ **RedirecciÃ³n AutomÃ¡tica**
Si haces una peticiÃ³n a cualquier nodo, Ã©l:
1. Calcula quÃ© nodo debe tener esos datos
2. Si NO es Ã©l, **redirige** automÃ¡ticamente al nodo correcto
3. El nodo correcto procesa y devuelve la respuesta

### 3ï¸âƒ£ **RÃ©plicas (Opcional)**
Cada nodo puede tener una **copia de respaldo** (rÃ©plica) que:
- Solo **lee** datos
- Se sincroniza automÃ¡ticamente con el nodo principal
- Si el principal falla, la rÃ©plica puede responder consultas

---

## ğŸ–¥ï¸ ConfiguraciÃ³n de 4 Computadoras

### **Requisitos**
- 4 PCs conectadas a la **misma red WiFi/LAN**
- Cada PC debe tener **Go instalado** (go.dev/download)
- Puerto **8080 abierto** en el firewall

---

## ğŸ“ Instrucciones para tu Equipo

### **PASO 1: PreparaciÃ³n en TODAS las PCs**

Cada persona debe:

1. **Instalar Go** desde https://go.dev/download/
   - Descargar el instalador `.msi` para Windows
   - Instalar con las opciones por defecto
   - Verificar: `go version`

2. **Obtener su IP**:
   ```powershell
   ipconfig | Select-String -Pattern "IPv4"
   ```
   Ejemplo: `192.168.1.10`

3. **Copiar el proyecto** completo a su PC (compartir por USB o Google Drive)

4. **Abrir PowerShell como Administrador** y ejecutar:
   ```powershell
   New-NetFirewallRule -DisplayName "Distributed DB" -Direction Inbound -Protocol TCP -LocalPort 8080 -Action Allow
   ```

---

### **PASO 2: Recopilar las IPs**

Anota las IPs de las 4 computadoras:

| Computadora | Nombre Nodo | IP              | Puerto | Responsable  |
|-------------|-------------|-----------------|--------|--------------|
| PC 1        | Mumbai      | 192.168.1.10    | 8080   | Persona A    |
| PC 2        | Delhi       | 192.168.1.11    | 8080   | Persona B    |
| PC 3        | Chennai     | 192.168.1.12    | 8080   | Persona C    |
| PC 4        | Bangalore   | 192.168.1.13    | 8080   | Persona D    |

*(Cambia las IPs por las reales)*

---

### **PASO 3: Editar la ConfiguraciÃ³n**

**EN TODAS LAS PCs**, editar el archivo `sharding-distributed.toml` con las IPs reales:

```toml
[[shards]]
name = "Mumbai"
idx = 0
address = "192.168.1.10:8080"    # IP de la PC 1
replicas = []

[[shards]]
name = "Delhi"
idx = 1
address = "192.168.1.11:8080"    # IP de la PC 2
replicas = []

[[shards]]
name = "Chennai"
idx = 2
address = "192.168.1.12:8080"    # IP de la PC 3
replicas = []

[[shards]]
name = "Bangalore"
idx = 3
address = "192.168.1.13:8080"    # IP de la PC 4
replicas = []
```

**âš ï¸ IMPORTANTE:** El archivo debe ser **idÃ©ntico** en las 4 PCs.

---

### **PASO 4: Compilar el Proyecto**

**EN TODAS LAS PCs**, abrir PowerShell normal en la carpeta del proyecto:

```powershell
cd "ruta\del\proyecto"
go mod download
go build
```

Esto crea el archivo `distributedKV.exe`

---

### **PASO 5: Ejecutar Cada Nodo**

Cada persona ejecuta **SU nodo** correspondiente:

**ğŸ“ En la PC 1 (Mumbai):**
```powershell
.\distributedKV.exe --db-location=Mumbai.db --http-addr=0.0.0.0:8080 --config-file=sharding-distributed.toml --shard=Mumbai
```

**ğŸ“ En la PC 2 (Delhi):**
```powershell
.\distributedKV.exe --db-location=Delhi.db --http-addr=0.0.0.0:8080 --config-file=sharding-distributed.toml --shard=Delhi
```

**ğŸ“ En la PC 3 (Chennai):**
```powershell
.\distributedKV.exe --db-location=Chennai.db --http-addr=0.0.0.0:8080 --config-file=sharding-distributed.toml --shard=Chennai
```

**ğŸ“ En la PC 4 (Bangalore):**
```powershell
.\distributedKV.exe --db-location=Bangalore.db --http-addr=0.0.0.0:8080 --config-file=sharding-distributed.toml --shard=Bangalore
```

**âœ… Si funciona**, verÃ¡s en cada consola:
```
2025/11/19 21:30:00 Shard count is 4, current shard: X
```

---

### **PASO 6: Probar la Base de Datos**

Desde **CUALQUIER PC**, puedes hacer peticiones:

#### **Guardar datos:**
```powershell
# Desde cualquier PC, apunta a CUALQUIER nodo (Mumbai en este ejemplo)
curl "http://192.168.1.10:8080/set?key=nombre&value=Juan"
curl "http://192.168.1.10:8080/set?key=edad&value=25"
curl "http://192.168.1.10:8080/set?key=ciudad&value=Madrid"
```

#### **Leer datos:**
```powershell
curl "http://192.168.1.10:8080/get?key=nombre"
# Respuesta: "Juan"

curl "http://192.168.1.10:8080/get?key=edad"
# Respuesta: "25"
```

#### **Lo que sucede internamente:**
```
1. Haces peticiÃ³n a Mumbai (192.168.1.10)
2. Mumbai calcula: hash("nombre") % 4 = 2
3. Mumbai ve que Shard 2 = Chennai (192.168.1.12)
4. Mumbai redirige automÃ¡ticamente a Chennai
5. Chennai guarda/obtiene el dato
6. Chennai responde
```

---

## ğŸ§ª Experimentos para Demostrar

### **Experimento 1: Ver la DistribuciÃ³n**
```powershell
# Guardar 10 claves
curl "http://192.168.1.10:8080/set?key=clave1&value=valor1"
curl "http://192.168.1.10:8080/set?key=clave2&value=valor2"
curl "http://192.168.1.10:8080/set?key=clave3&value=valor3"
# ... hasta clave10

# Observar en cada consola quÃ© nodo guardÃ³ quÃ© datos
# Los datos se reparten automÃ¡ticamente entre los 4 nodos
```

### **Experimento 2: Tolerancia a Fallos**
```powershell
# 1. Guardar un dato
curl "http://192.168.1.10:8080/set?key=test&value=funciona"

# 2. Apagar UNA de las PCs (ej: Chennai)

# 3. Intentar leer un dato que estaba en Chennai
curl "http://192.168.1.10:8080/get?key=test"
# Si "test" estaba en Chennai, darÃ¡ error

# 4. Leer datos de otros nodos (siguen funcionando)
curl "http://192.168.1.10:8080/get?key=clave1"
# Si "clave1" estaba en Mumbai/Delhi/Bangalore, funciona perfectamente
```

### **Experimento 3: Balanceo de Carga**
```powershell
# Hacer peticiones a diferentes nodos
curl "http://192.168.1.10:8080/set?key=a&value=1"  # A Mumbai
curl "http://192.168.1.11:8080/set?key=b&value=2"  # A Delhi
curl "http://192.168.1.12:8080/set?key=c&value=3"  # A Chennai

# Todos redirigen correctamente segÃºn el hash
# Observar en las consolas las redirecciones
```

---

## ğŸ” CÃ³mo Funciona Internamente

### **Algoritmo de Sharding:**
```go
// En el cÃ³digo (config/config.go)
func (s *Shards) Index(key string) int {
    h := fnv.New64()           // FunciÃ³n hash
    h.Write([]byte(key))       // Hashear la clave
    return int(h.Sum64() % uint64(s.Count))  // MÃ³dulo para obtener el shard
}
```

**Ejemplo:**
- `hash("nombre")` = `12847563982765432`
- `12847563982765432 % 4` = `2` â†’ **Shard 2 (Chennai)**

### **Flujo de una peticiÃ³n SET:**
```
Cliente â†’ Nodo A â†’ Â¿Es mi shard? 
                    â”œâ”€ SÃ â†’ Guardar localmente
                    â””â”€ NO â†’ HTTP Redirect â†’ Nodo B â†’ Guardar
```

### **Archivos importantes:**
- `main.go` - Punto de entrada, parsea argumentos
- `config/config.go` - Lee configuraciÃ³n y calcula sharding
- `db/db.go` - InteractÃºa con la base de datos local (BoltDB)
- `web/web.go` - Maneja peticiones HTTP y redirecciones
- `replication/replication.go` - Sincroniza rÃ©plicas

---

## â“ Preguntas Frecuentes

**P: Â¿Por quÃ© usar `0.0.0.0:8080` en lugar de `127.0.0.1`?**  
R: `0.0.0.0` escucha en TODAS las interfaces de red (permite conexiones externas). `127.0.0.1` solo acepta conexiones locales.

**P: Â¿QuÃ© pasa si una PC se desconecta?**  
R: Los datos que estaban en ese nodo NO estarÃ¡n disponibles, pero el resto del sistema sigue funcionando. Por eso existen las rÃ©plicas.

**P: Â¿CÃ³mo sÃ© quÃ© datos estÃ¡n en cada nodo?**  
R: Cada nodo crea un archivo `.db` (Mumbai.db, Delhi.db, etc.) con sus datos. El tamaÃ±o del archivo indica cuÃ¡ntos datos tiene.

**P: Â¿Puedo usar solo 2 o 3 PCs?**  
R: SÃ­, edita `sharding-distributed.toml` y reduce el nÃºmero de shards. Pero necesitas al menos 2 para demostrar distribuciÃ³n.

**P: Â¿Por quÃ© a veces da error "connection refused"?**  
R: Verifica:
1. Firewall abierto en todas las PCs
2. Todas las PCs ejecutando su nodo
3. IPs correctas en `sharding-distributed.toml`
4. Todas en la misma red WiFi/LAN

---

## ğŸ“Š Para la PresentaciÃ³n

**Demuestra:**
1. âœ… **Sharding**: Los datos se reparten entre nodos
2. âœ… **Escalabilidad**: Agregar mÃ¡s nodos distribuye mÃ¡s la carga
3. âœ… **RedirecciÃ³n**: Cualquier nodo puede recibir cualquier peticiÃ³n
4. âœ… **Disponibilidad parcial**: Si cae un nodo, los demÃ¡s siguen funcionando

**Conceptos tÃ©cnicos:**
- Consistent Hashing (hash + mÃ³dulo)
- HTTP redirects (307 Temporary Redirect)
- Key-Value Store (BoltDB)
- Concurrent requests (goroutines de Go)

---

## ğŸ¯ Checklist Final

- [ ] Go instalado en las 4 PCs
- [ ] IPs obtenidas de las 4 PCs
- [ ] `sharding-distributed.toml` editado con IPs reales (IGUAL en todas)
- [ ] Proyecto copiado a las 4 PCs
- [ ] Puerto 8080 abierto en firewall (las 4 PCs)
- [ ] `go build` ejecutado en las 4 PCs
- [ ] Cada nodo ejecutÃ¡ndose en su PC correspondiente
- [ ] Prueba exitosa: `curl "http://IP:8080/set?key=test&value=ok"`
- [ ] VerificaciÃ³n: Los datos se distribuyen entre los 4 nodos

---

**Â¡Buena suerte con la presentaciÃ³n! ğŸš€**
